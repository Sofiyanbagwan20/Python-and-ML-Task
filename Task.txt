Scenario 1: Data ValidationTask: Write a function validate_data(data) that checks if a list of dictionaries (e.g., [{"name": "Alice", "age": 30}, {"name": "Bob", "age": "25"}]) contains valid integer values for the "age" key. Return a list of invalid entries.


def validate_data(data):
    invalid = []

    for d in data:
        if not isinstance(d.get("age"), int):
            invalid.append(d)

    return invalid

.........................................................................................................

    Scenario 2: Logging DecoratorTask: Create a decorator @log_execution_time that logs the time taken to execute a function. Use it to log the runtime of a sample function calculate_sum(n) that returns the sum of numbers from 1 to n.


@log_execution_time
def calculate_sum(n):
    total = 0
    for i in range(1, n + 1):
        total += i
    return total

..........................................................................................................

    Scenario 3: Missing Value Handling
Task: A dataset has missing values in the "income" column. Write code to:

1. Replace missing values with the median if the data is normally distributed.

2. Replace with the mode if skewed.
Use Pandas and a skewness threshold of 0.5.

import pandas as pd


data = {
    "income": [30000, 35000, None, 40000, None, 45000, 100000]
}

df = pd.DataFrame(data)


skewness = df["income"].skew()


if abs(skewness) <= 0.5:
    df["income"].fillna(df["income"].median(), inplace=True)
else:
    df["income"].fillna(df["income"].mode()[0], inplace=True)

print(df)


........................................................................................................

Scenario 4: Text Pre-processing
Task: Clean a text column in a DataFrame by:

1. Converting to lowercase.

2. Removing special characters (e.g., !, @).

3. Tokenizing the text.


import pandas as pd
import re


data = {
    "text": ["Hello World!", "AI & ML @ Python", "Data-Science is FUN!!!"]
}

df = pd.DataFrame(data)

# Text cleaning
df["clean_text"] = df["text"].str.lower() \
    .apply(lambda x: re.sub(r'[^a-z\s]', '', x)) \
    .apply(lambda x: x.split())

print(df)

......................................................................................................

Scenario 5: Hyperparameter Tuning
Task: Use GridSearchCV to find the best max_depth (values: [3, 5, 7]) and n_estimators (values: [50, 100]) for a Random Forest classifier.

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)


rf = RandomForestClassifier(random_state=42)


param_grid = {
    "max_depth": [3, 5, 7],
    "n_estimators": [50, 100]
}


grid = GridSearchCV(
    rf,
    param_grid,
    cv=5
)


grid.fit(X, y)


print("Best Parameters:", grid.best_params_)

........................................................................................................


Scenario 6: Custom Evaluation Metric
Task: Implement a custom metric weighted_accuracy where class 0 has a weight of 1 and class 1 has a weight of 2.


def weighted_accuracy(y_true, y_pred):
    correct = 0
    total = 0

    for i in range(len(y_true)):
        if y_true[i] == 1:
            weight = 2
        else:
            weight = 1

        total += weight

        if y_true[i] == y_pred[i]:
            correct += weight

    return correct / total

    ..................................................................................................

    Scenario 7: Image Augmentation
Task: Use TensorFlow/Keras to create an image augmentation pipeline with random rotations (±20 degrees), horizontal flips, and zoom (0.2x).


import tensorflow as tf
from tensorflow.keras import layers

data_augmentation = tf.keras.Sequential([
    layers.RandomRotation(0.055),   # ~20 degrees
    layers.RandomFlip("horizontal"),
    layers.RandomZoom(0.2)
])


.....................................................................................................

Scenario 8: Model Callbacks
Task: Implement an EarlyStopping callback that stops training if validation loss doesn’t improve for 3 epochs and restores the best weights.

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

......................................................................................................

Scenario 9: Structured Response Generation
Task: Use the Gemini API to generate a response in JSON format for the query: "List 3 benefits of Python for data science." Handle cases where the response isn’t valid JSON.


import json

gemini_response = '''
{
  "benefits": [
    "Easy to learn and use",
    "Large data science libraries",
    "Strong community support"
  ]
}
'''

try:
    result = json.loads(gemini_response)
    print(result)
except json.JSONDecodeError:
    print("Response is not valid JSON")


...........................................................................................................

Scenario 10: Summarization with Constraints
Task: Write a prompt to summarize a news article into 2 sentences. If the summary exceeds 50 words, truncate it to the nearest complete sentence.

Summarize the following news article in exactly 2 sentences.
Keep the summary concise and informative.

If the total summary exceeds 50 words, truncate it to the nearest complete sentence
so that it stays within 50 words.

Article:
<PASTE NEWS ARTICLE HERE>


............................................................................................................